---
title: "Final Project"
author: "Jackson Waschura, Bryan Liu, Camille Garlick, Eshley Freed-Doerr"
date: "6/6/2021"
output: 
  prettydoc::html_pretty:
    theme: hpstr
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center', message = FALSE, warning = FALSE, fig.width=12, fig.height=5)
```

```{r}
library(readxl)
library(tidyverse)
library(gridExtra)
library(gganimate)
library(gapminder)
```

# Understanding US School Expenditures and Test Scores

Gaps in educational success due to numerous factors such as socioeconomic class, race, sex, instruction expenditure, and more has been studied for decades. Today, computer algorithms allow the data to be analyzed more efficiently, and machine learning can be harnessed to make predictions on future data based off the knowledge regarding the observed data. Regression analysis proves to be an efficacious tool in detecting which factors are statistically significant in determining exam scores and extrapolating to predict future patterns. 

In order to understand how school expenditures and various diversity indicators factor into students' test scores, we examine a dataset provided [here](https://www.kaggle.com/noriuk/us-education-datasets-unification-project). We cleaned the dataset and split observations into distinct geographical regions to ensure a statistically significant data quantity in each region. Splitting the data into geographical regions can be very informative, as geographical regions are often correlates for political views, major industries, etc. that can be investigated more in future.

<center>
![Region Divisions](usmap.png)
</center>

```{r}
# DATA GET AND CLEANING PART 1
schools <- read_excel(here::here("data", "US_schools_data.xlsx"))
```

```{r}
# Select the relevant columns
schools <- schools %>% 
  select(PRIMARY_KEY, ends_with(c("EXPENDITURE", "READING", "MATHEMATICS")))
```

```{r}
# Split primary key
schools <- schools %>% 
  mutate(
    YEAR = as.integer(str_extract(PRIMARY_KEY, "^\\d{4}")),
    STATE = str_extract(PRIMARY_KEY, "(?<=\\d{4}_).*")
  ) %>%
  select(YEAR, STATE, TOTAL_EXPENDITURE:G08_TR_A_MATHEMATICS) 
```

```{r}
# Pivot longer
schools <- schools %>% 
  pivot_longer(
    cols = ends_with(c("MATHEMATICS", "READING")),
    names_to = c("GRADE", "RACE", "SEX", "TEST"),
    values_to = "SCORE",
    names_sep = "_"
  )
```

```{r}
# Seperate into regions
# Note: DODEA stands for "Department of Defense Education Activity," which are 
# DoD sponsored schools located in Virginia. Due to this, DODEA is put into 
# the same geographic region as Virginia.
# Additionally, National is grouped into its own region.

northeast <- c("MAINE", "NEW_HAMPHSIRE", "VERMONT", "MASSACHUSETTS", "RHODE_ISLAND", "CONNECTICUT", "NEW_YORK", "NEW_JERSEY", "PENNSYLVANIA")
midwest <- c("OHIO", "MICHIGAN", "INDIANA", "WISCONSIN", "ILLINOIS", "MINNESOTA", "IOWA", "MISSOURI", "NORTH_DAKOTA", "SOUTH_DAKOTA", "NEBRASKA", "KANSAS")
south <- c("DELAWARE", "MARYLAND", "VIRGINIA", "WEST_VIRGINIA", "KENTUCKY", "NORTH_CAROLINA", "SOUTH_CAROLINA", "TENNESSEE", "GEORGIA", "FLORIDA", "ALABAMA", "MISSISSIPPI", "ARKANSAS", "LOUISIANA", "TEXAS", "OKLAHOMA", "DISTRICT_OF_COLUMBIA")
west <- c("MONTANA", "IDAHO", "WYOMING", "COLORADO", "NEW_MEXICO", "ARIZONA", "UTAH", "NEVADA", "CALIFORNIA", "OREGON", "WASHINGTON", "ALASKA", "HAWAII")


schools <- schools %>%
  mutate(
    REGION = as.factor(case_when(
      STATE %in% northeast ~ "Northeast",
      STATE %in% midwest ~ "Midwest",
      STATE %in% south ~ "South",
      STATE %in% west ~ "West",
      STATE=="NATIONAL" ~ "NATIONAL"
    )),
    SEX = as.factor(SEX),
    RACE = as.factor(RACE)
  )
```

```{r, include = FALSE}
schools %>% distinct(RACE)
```

```{r}
# Additional cleaning here - make data better for visualising
schools <- schools %>%
  filter(!is.na(SCORE)) %>%
  filter(REGION != "NATIONAL") %>%
  mutate(RACE=fct_recode(RACE,
                         All="A",
                         White="WH",
                         Black="BL",
                         Hispanic="HI",
                         Asian="AS",
                         `American Indian`="AM",
                         `Pacific Islander`="HP",
                         `Two or more races`="TR"),
         SEX = fct_recode(SEX, All="A", Male="M", Female="F"),
         GRADE = fct_recode(GRADE, `8th Grade`="G08", `4th Grade`="G04"))
```

## Defining the Model 

We proceed to examine the relationship between instructional expenditures and testing scores in the context of these geographical regions. The scores don't seem to vary much by region at all, as can be seen by the similar spread of data points on the y-axis of each visualisation, but the instruction expenditures widely vary depending on the region, indicating some relationship between instruction expenditures and geographical region, as could be expected. The midwest and south tend to have lower expenditures, while the northeast and west have much wider spreads of higher and lower expenditure values. 


```{r, fig.height = 5}
# DATA VISUALISATION PART 1
# Excluding National: nothing significant about National here

schools %>%
  filter(REGION!="NATIONAL", !is.na(INSTRUCTION_EXPENDITURE) & !is.na(SCORE)) %>%
  ggplot(mapping=aes(color=REGION)) +
  geom_point(mapping=aes(y=SCORE, x=INSTRUCTION_EXPENDITURE), show.legend=FALSE) +
  facet_wrap(~ REGION, ncol = 2) +
  labs(title = "Instructional Expenditures and Testing Scores by Region", x = "Instruction Expenditure by State in $")
```

The data visualized above is including two distinct types of tests and two distinct ages of students. To glean more about the differences in how students perform depending on their age and test, we visualize the distribution of scores based on age and test below. 

```{r}
schools %>%
  filter(RACE=="All", SEX=="All") %>%
  ggplot(aes(x=SCORE, fill=TEST)) + 
  geom_histogram(alpha=0.6, position="identity", bins = 50) +
  facet_wrap(~GRADE) +
  labs(title = "Distribution of Math and Reading Test Scores", x = "Instruction Expenditure by State in $")
```


An additional factor we may wish to consider is how our distribution of scores and expenditures by region might be changing over time. Below we examine the data in the context of decades to explore how the relationship between these variables are changing over time.


```{r, fig.width=10}
schools %>%
  filter(REGION!="NATIONAL", !is.na(INSTRUCTION_EXPENDITURE) & !is.na(SCORE)) %>%
  mutate(
    DECADE = floor(YEAR/10) * 10
  ) %>%
  ggplot(mapping=aes(color=REGION)) +
  geom_point(mapping=aes(y=SCORE, x=INSTRUCTION_EXPENDITURE), show.legend=FALSE) +
  facet_grid(REGION ~ DECADE) +
  labs(title = "School Instructional Expenditures and Test Scores Over Time by Region", x = "Instruction Expenditure by State in $")
```

For each region, the spread of data across expenditures is clearly expanding and scores are getting higher. There is still no clear relationship between expenditure and score, but we absolutely see indication that depending on the region, and depending on the decade, we can more reliably predict what kind of data we see.

To confirm our suspicion that there is not a significant relationship between instruction expenditure and test scores, we use a linear regression model to calculate the r-squared value:

```{r}
# LINEAR REGRESSION SECTION IN PART 1
# Modeling both tests

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE, data=schools)
cat("R-squared with instruction expenditure and tests as independent variables: ", broom::glance(schools.lm)$r.squared)
```
The r-squared value is quite low for our model, at an approximately 12% fit, indicating that instruction expenditure is not a good fit for predicting score, as we suspected. Let's see whether this changes at all if we separate our tests. First we'll investigate if the r-squared changes if we only look at math tests:

```{r}
# Model Mathematics

schools.lm = lm(SCORE~INSTRUCTION_EXPENDITURE,
                data=filter(.data=schools, TEST == "MATHEMATICS"))
cat("R-squared with mathematics test: ", broom::glance(schools.lm)$r.squared)
```
And now we'll look at reading tests:

```{r}
# Model Reading
schools.lm = lm(SCORE~INSTRUCTION_EXPENDITURE,
                data=filter(.data=schools, TEST == "READING"))
cat("R-squared with reading test: ", broom::glance(schools.lm)$r.squared)
```
The r-squared values get even smaller when we divide the tests into distinct math scores and reading scores, indicating that instruction expenditures may be slightly predictive of scores as a whole, but certainly could not be used to predict a typical students' individual math or reading score.

So, let's start to identify what type of model might work better! We saw in our visualisations that the decade seemed to impact our visualisation, so let's include the year and calculate the r-squared.

```{r}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR, data=schools)
cat("R-squared with both tests and year: ", broom::glance(schools.lm)$r.squared)
```
The r-squared definitely changes, but it's still not the best model, as our independent variables explain merely 15% of the variability in score. So, as we add more variables to identify a good regression model to use, we will switch to using adjusted R-squared, which can be a better indicator of how predictive a model is when there are many independent variables being considered. We identify the best model by adding in variables to our model one at a time, and taking our best-fitting variable as our current model and then going back to adding each predictor one at a time. The predictors we will consider including now are the grade, sex, race of student along with the region.


```{r, include=FALSE}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR, data=schools)
broom::glance(schools.lm)$adj.r.squared
```

```{r, include=FALSE}
# FINDING THE BEST MODEL
# First iteration: grade is best variable
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+GRADE, data=schools)
broom::glance(schools.lm)$adj.r.squared
```
```{r, include=FALSE}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+SEX, data=schools)
broom::glance(schools.lm)$adj.r.squared
```
```{r, include=FALSE}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+RACE, data=schools)
broom::glance(schools.lm)$adj.r.squared
```
```{r, include=FALSE}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+REGION, data=schools)
broom::glance(schools.lm)$adj.r.squared
```

```{r, include=FALSE}
# Next iteration: Race is best variable
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+GRADE+SEX, data=schools)
broom::glance(schools.lm)$adj.r.squared
```
```{r, include=FALSE}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+GRADE+RACE, data=schools)
broom::glance(schools.lm)$adj.r.squared
```
```{r, include=FALSE}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+GRADE+REGION, data=schools)
broom::glance(schools.lm)$adj.r.squared
```

```{r, include=FALSE}
# Next iteration: Region is best variable
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+GRADE+RACE+SEX, data=schools)
broom::glance(schools.lm)$adj.r.squared
```
```{r, include=FALSE}
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+GRADE+RACE+REGION, data=schools)
broom::glance(schools.lm)$adj.r.squared
```

After this iterative process of determine which regression model works best, the final model we chose included the test, instruction expenditure, year, grade, race, region, and sex as predictors. It achieved the maximal adjusted R squared seen below at 0.9215.

```{r}
# Final iteration:  sex is still good variable
# Modeling with additional variables

schools.lm = lm(SCORE~TEST+INSTRUCTION_EXPENDITURE+YEAR+GRADE+RACE+REGION+SEX, data=schools)
cat("Adjusted r-squared: ", broom::glance(schools.lm)$adj.r.squared)
```

## Visualizing and Interpreting the Model 

We have our ideal regression model, so let's visualize the relationships between the variables in this model. The graphs below show relationships between the independent variables and the score. Each graph separates the regression lines and data points by grade, as eighth graders consistently score much higher than fourth graders. 


```{r, fig.width=12}
schools %>% 
  filter(SEX != "A") %>% 
  ggplot(gapminder, mapping = aes(x=INSTRUCTION_EXPENDITURE, y=SCORE, color=GRADE)) +
  facet_grid(TEST~REGION) +
  geom_point() +
  geom_smooth(method = "lm", mapping = aes()) +
  labs(title = "Test Score and Expenditure Relationships By Grade and Region", 
       subtitle = "Year: {frame_time}", x = "Instruction Expenditure by State in $") +
  theme(panel.spacing.x = unit(6, "mm"), panel.spacing.y = unit(6, "mm")) +
  transition_time(YEAR) +
  ease_aes('linear')
```


Our first graph shows how each region's scores tend to get higher over time, and instruction expenditure increases. 

```{r, fig.width=12}
schools %>% 
  filter(RACE != "All") %>% 
  ggplot(mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SCORE, color=GRADE)) +
  facet_grid(TEST~RACE) +
  geom_point() +
  geom_smooth(method = "lm", mapping=aes()) + 
  scale_x_continuous(n.breaks = 3) +
  labs(title = "Test Score and Expenditure Relationships By Grade and Race", 
       x = "Instruction Expenditure by State in $") +
  theme(panel.spacing.x = unit(6, "mm"), panel.spacing.y = unit(6, "mm"))
```
The next graph is faceted by race, and we clearly see a clear relationship between race, score, and instruction expenditure, where the race is predictive of the spread of instruction expenditure and how high/low test scores are in students. 

```{r, fig.width=12}
schools %>% 
  filter(SEX != "All") %>% 
  ggplot(mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SCORE, color=GRADE)) +
  facet_grid(TEST~SEX) +
  geom_point() +
  geom_smooth(method = "lm", mapping=aes()) + 
  scale_x_continuous(n.breaks = 3) +
  labs(title = "Test Score and Expenditure Relationships By Grade and Sex", 
       x = "Instruction Expenditure by State in $") +
  theme(panel.spacing.x = unit(6, "mm"), panel.spacing.y = unit(6, "mm")) 
```

Lastly, in the above graph we look at how sex is related to our other variables. While there are small differences between the sex, where females seem to score slightly higher on Reading and males seem to score slightly higher on Mathematics, sex doesn't seem to be as strong a predictor.

## Validating model through generation of simulated data

Now we'll use our model to make a simulated dataset, in order to compare the observed data with the simulation. We'll explore the simulated dataset with the same graphs we just used above.

```{r}
# SIMULATION AND PREDICTIVE CHECKS SECTION
schools <- schools %>% 
  mutate(
  SIM_SCORE = predict(schools.lm, newdata = schools)
               + rnorm(length(schools), sd=sqrt(sigma(schools.lm)))
  )
```

```{r, fig.width=12}
schools %>% 
  filter(SEX != "A") %>% 
  ggplot(gapminder, mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SIM_SCORE, color=GRADE)) +
  facet_grid(TEST~REGION) +
  geom_point() +
  geom_smooth(method = "lm", mapping=aes()) +
  labs(title = "Test Score and Expenditure Relationships By Grade and Region", 
       subtitle = "Year: {frame_time}", x = "Instruction Expenditure by State in $") +
  theme(panel.spacing.x = unit(6, "mm"), panel.spacing.y = unit(6, "mm")) +
  transition_time(YEAR) +
  ease_aes('linear')
```

```{r, fig.width=12}
schools %>% filter(RACE != "All") %>% ggplot(mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SIM_SCORE, color=GRADE)) +
  facet_grid(TEST~RACE) +
  geom_point() +
  geom_smooth(method = "lm", mapping=aes()) + 
  scale_x_continuous(n.breaks = 3) +
  labs(title = "Test Score and Expenditure Relationships By Grade and Race", 
       x = "Instruction Expenditure by State in $") +
  theme(panel.spacing.x = unit(6, "mm"), panel.spacing.y = unit(6, "mm"))
```

```{r, fig.width=12}
schools %>% filter(SEX != "All") %>% ggplot(mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SIM_SCORE, color=GRADE)) +
  facet_grid(TEST~SEX) +
  geom_point() +
  geom_smooth(method = "lm", mapping=aes()) + 
  scale_x_continuous(n.breaks = 3) +
  labs(title = "Test Score and Expenditure Relationships By Grade and Sex", 
       x = "Instruction Expenditure by State in $") +
  theme(panel.spacing.x = unit(6, "mm"), panel.spacing.y = unit(6, "mm"))
```

The graphs above are very similar to the distributions we saw before. The sets of graphs look very similar, though the simulated scored tend to have less variability, which is good as it indicates our model is reliably predictive.

Let's compare our dataset side-by-side to see precisely how different our simulated dataset is from our observed data.

```{r}
# The below section is hugely based on this article: (here)[https://statisticsglobe.com/add-common-legend-to-combined-ggplot2-plots-in-r/]

# The actual plots
obs_plot <- schools %>% 
  ggplot(mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SCORE, color=GRADE)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_y_continuous(limits=c(180, 320)) +
  labs(title = "Observed Scores",
       x = "Instruction Expenditure by State in $") +
  theme(legend.position = "None")

sim_plot <- schools %>% 
  ggplot(mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SIM_SCORE, color=GRADE)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_y_continuous(limits=c(180, 320)) +
  labs(title = "Simulated Scores", 
       x = "Instruction Expenditure by State in $") +
  theme(legend.position = "None")

# Below plot is just to create legend

legend_sim_plot <- schools %>% 
  ggplot(mapping=aes(x=INSTRUCTION_EXPENDITURE, y=SIM_SCORE, color=GRADE)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_y_continuous(limits=c(180, 320)) +
  labs(title = "Simulated Scores", 
       x = "Instruction Expenditure by State in $") +
  theme(legend.position = "bottom")

# Function (used a function found online)

extract_legend <- function(my_ggp) {
  step1 <- ggplot_gtable(ggplot_build(my_ggp))
  step2 <- which(sapply(step1$grobs, function(x) x$name) == "guide-box")
  step3 <- step1$grobs[[step2]]
  return(step3)
}

legend <- extract_legend(legend_sim_plot)

#Arrange

grid.arrange(arrangeGrob(obs_plot, sim_plot, ncol = 2),
             legend, nrow = 2)
```
As we saw from our graphs above, the distributions are very similar, though there are vastly fewer outliers in the simulation. Because the simulated scores were generated with a constant variance, the simulated data appears more compact (lower spread, less fanning-out). As a result, it is expected that fewer outliers will be observed in the simulated data.

Since we have a reliable simulation, we create 1000 simulations and regress the observed data against each simulated data. We then plot the R^2 of these regressions below.

```{r}
# GENERATING MULTIPLE PREDICTIVE CHECKS
get_sim_r_sq <- function(n){
  simulated <- predict(schools.lm, newdata = schools) +
    rnorm(length(schools), sd=sqrt(sigma(schools.lm)))
  return(broom::glance(lm(schools$SCORE ~ simulated))$r.squared)
}

n_sims <- 1000
simulation_results <- tibble(`R Squared`=as_vector(map(1:n_sims, get_sim_r_sq)))
```

```{r}
mean_r_sq <- mean(simulation_results$`R Squared`)

simulation_results %>%
  ggplot(mapping = aes(x=`R Squared`)) +
  geom_histogram(bins=50) +
  geom_vline(xintercept=mean_r_sq, color="Orange") +
  annotate("text", x=mean_r_sq+0.0015, y=n_sims*0.025,
           label=format(mean_r_sq, digits=3), color="Orange", size=5) +
  labs(title = "Distribution of Percent of Observed Variation explained by Model")
```

The R-squared is an indication for how well the simulation fits the observed data, and as the graph shows, are simulations have a mean fit of 91.2%, and the vast majority of simulations with an R-squared score of above 90%. This high value of R^2 indicates that the model is able to capture most of the variance in the original dataset with the simulated data that it produces. 

Our multivariate linear regression model, having been validated by our many simulations, will produce a simulated dataset that well-represents student test scores across mathematics and reading.